# --- rename to .env and fill in ---

# Google Gemini (cloud) inference
GOOGLE_API_KEY=
GEMINI_MODEL_TEXT=gemini-2.5-pro
GEMINI_MODEL_IMAGE=gemini-2.5-flash-image

# Local inference (set USE_LOCAL_INFERENCE=true to default to local)
USE_LOCAL_INFERENCE=true

# Ollama text generation
LOCAL_LLM_URL=http://localhost:11434/api/generate
LOCAL_LLM_MODEL=gpt-oss:20b

# Local portrait generation (HTTP fallback endpoint)
LOCAL_PORTRAIT_URL=http://localhost:7860/generate

# Diffusers model id for direct local image generation (Flux)
LOCAL_IMAGE_MODEL=black-forest-labs/FLUX.1-schnell

# Optional parameters for local image generation
# Steps: higher is slower but higher quality
LOCAL_IMAGE_STEPS=4
# CFG guidance scale
LOCAL_IMAGE_GUIDANCE=0.0
# Seed: set >=0 for deterministic results
LOCAL_IMAGE_SEED=42
# Dimensions (0 uses model default)
LOCAL_IMAGE_WIDTH=0
LOCAL_IMAGE_HEIGHT=0

# On macOS, prefer MPS; do not force CPU fallback
PYTORCH_ENABLE_MPS_FALLBACK=1

# Logging
LOG_LEVEL=INFO

# Rules API proxy (5e SRD)
RULES_BASE_URL=https://www.dnd5eapi.co
RULES_API_PREFIX=api/2014

# Server ports
PORT_API=8000
PORT_WEB=5173

# Frontend (Vite)
# Vite web app reads API port from here
VITE_API_PORT=8000
# Optional: used only to enable Quick NPC backstory button by default
VITE_GOOGLE_API_KEY=
